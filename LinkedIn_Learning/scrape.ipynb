{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "#request library houses the get function, we'll use to ping the web address\n",
    "#this process returns a response contatining the source code from the website\n",
    "from bs4 import BeautifulSoup\n",
    "#programs work in unity to create a navigable object that we can use to isolate specific data we want from the website\n",
    "url='https://quotes.toscrape.com/'\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text, 'lxml') #parsing response's test attribute using lxml parser\n",
    "\n",
    "#searching for all elements with the span tag and the class text, we can obtain quote info\n",
    "quotes=soup.find_all('span', class_='text')\n",
    "#find_all function hones in on the tag and class specified\n",
    "#print only text property by looping through each quote's html\n",
    "authors=soup.find_all('small', class_='author')\n",
    "tags=soup.find_all('div', class_='tags')\n",
    "\n",
    "for i in range(0, len(quotes)):\n",
    "    print(f\"{quotes[i].text}, {authors[i].text}\")\n",
    "    quoteTags=tags[i].find_all('a', class_='tag')\n",
    "    for quoteTag in quoteTags:\n",
    "        print(quoteTag.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url2='https://scrapingclub.com/exercise/list_basic/'\n",
    "response=requests.get(url2)\n",
    "soup=BeautifulSoup(response.text, 'lxml')\n",
    "items=soup.find_all('div', class_='col-lg-4 col-md-6 mb-4')\n",
    "count=1\n",
    "for i in items:\n",
    "    itemName=i.find('h4', class_='card-title').text.strip('\\n')\n",
    "    itemPrice=i.find('h5').text\n",
    "    print(f\"{count}, Price: {itemPrice}, Item Name: {itemName}\")\n",
    "    count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url='https://scrapingclub.com/exercise/list_basic/'\n",
    "response=requests.get(url2)\n",
    "soup=BeautifulSoup(response.text, 'lxml')\n",
    "items=soup.find_all('div', class_='col-lg-4 col-md-6 mb-4')\n",
    "count=1\n",
    "for i in items:\n",
    "    itemName=i.find('h4', class_='card-title').text.strip('\\n')\n",
    "    itemPrice=i.find('h5').text\n",
    "    print(f\"{count}. Price: {itemPrice}, Item Name: {itemName}\")\n",
    "    count=count+1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pages=soup.find('ul', class_='pagination')\n",
    "urls=[]\n",
    "links=pages.find_all('a', class_='page-link')\n",
    "for link in links:\n",
    "    pageNum=int(link.text) if link.text.isdigit() else None\n",
    "    if pageNum!= None:\n",
    "        x=link.get('href')\n",
    "        urls.append(x)\n",
    "print(urls)\n",
    "count=1\n",
    "for i in urls:\n",
    "    newUrl=url+i\n",
    "    response =requests.get(newUrl)\n",
    "    soup=BeautifulSoup(response.text, 'lxml')\n",
    "    items=soup.find_all('div', class_='col-lg-4 col-md-6 mb-4')\n",
    "    for i in items:\n",
    "        itemName=i.find('h4', class_='card-title').text.strip('\\n')\n",
    "        itemPrice=i.find('h5').text\n",
    "        print(f\"{count}. Price: {itemPrice}, Item Name: {itemName}\")\n",
    "        count=count+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are ready to scrape information from a webpage, what is the best strategy to isolate the data you need from the webpage?\n",
    "\n",
    "* Look at the source code and find patterns in the HTML tags that contain the data you are looking for.\n",
    "\n",
    "What is the correct command to read each div HTML tag with a class called amount from a webpage?\n",
    "\n",
    "* amounts = soup.find_all('div', class_='amount')\n",
    "\n",
    "Sometimes your response query will pull all the data you need, plus some additional characters you do not need. How do you remove an unwanted character from a scraped line of text?\n",
    "\n",
    "* strip('\\n')\n",
    "\n",
    "Which library does Python use for web scraping?\n",
    "\n",
    "* Beautiful Soup\n",
    "\n",
    "You have an HTML response that you want to make more navigable and readable. Which command accomplishes this task?\n",
    "\n",
    "* soup = BeautifulSoup(response.txt, 'lxml')\n",
    "\n",
    "To prepare a webpage for scraping, you first need to convert the HTML code into a navigable object in Python. What is the correct command to retrieve source code from a webpage?\n",
    "\n",
    "* response = requests.get(url)\n",
    "\n",
    "When you encounter a webpage with pagination, which problem do you want to avoid when scraping the data?\n",
    "\n",
    "* capturing multiple links that go to the same page, which will create duplicate data in the scraped output\n",
    "\n",
    "A webpage often contains hyperlinks to other pages. These links may be important to scrape later. How would you use Python to parse webpage links inside an HTML response?\n",
    "\n",
    "* links = pages.find_all('a', class_='page-link')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51e4f816afd506db7bb7650607a3e2e026e40398beddcc9e11824953f6978b8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

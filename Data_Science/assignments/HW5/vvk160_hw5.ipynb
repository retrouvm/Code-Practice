{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 (100 + 20 points)\n",
    "* Download the file vvk160_hw5.ipynb\n",
    "* Open the file via Jupyter Notebook\n",
    "* Run the code by clicking the “Run” button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "from math import log2\n",
    "from csv import reader\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import r2_score as r2\n",
    "import pdb #for debugging the code. Set a breakpoint by pbd.set_trace(). https://www.geeksforgeeks.org/python-debugger-python-pdb/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Linear regression (20pts)\n",
    "* We do the linear regression on three points (0.5, 1), (2, 2.5), and (3, 3). \n",
    "* Please calculate the SSEs of the four following linear regression based on the definition SSE = ∑ ( y - ŷ )^2\n",
    "* Write the Python code to output the major steps of the calculation and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = x + 0.5, SSE = 0.25\n",
      "y = x + 1, SSE = 1.5\n",
      "y = 0.8*x + 0.3 + 15, SSE = 0.54\n",
      "y = 0.8*x + 0.7, SSE = 0.06\n",
      "\n",
      "A least squares regression selects the line with the lowest total sum of squared prediction errors. Thus y = 0.8*x + 0.7 is the best linear regression\n"
     ]
    }
   ],
   "source": [
    "# Which is the best linear regression using SSE?\n",
    "\n",
    "#y = x + 0.5\n",
    "SSE1=((1 - (0.5+0.5))**2)+((2.5 - (2+0.5))**2) +((3- (3+0.5))**2)\n",
    "print(f\"y = x + 0.5, SSE = {SSE1}\")\n",
    "\n",
    "#y = x + 1\n",
    "SSE2=((1 - (0.5+1))**2)+((2.5 - (2+1))**2) +((3- (3+1))**2)\n",
    "print(f\"y = x + 1, SSE = {SSE2}\")\n",
    "\n",
    "#y = 0.8*x + 0.3\n",
    "SSE3=((1 - (0.8*0.5+0.3))**2)+((2.5 - (0.8*2+0.3))**2) +((3- (0.8*3+0.3))**2)\n",
    "print(f\"y = 0.8*x + 0.3 + 15, SSE = {round(SSE3,3)}\")\n",
    "\n",
    "#y = 0.8*x + 0.7\n",
    "SSE4=((1 - (0.8*0.5+0.7))**2)+((2.5 - (0.8*2+0.7))**2) +((3- (0.8*3+0.7))**2)\n",
    "print(f\"y = 0.8*x + 0.7, SSE = {round(SSE4,3)}\")\n",
    "\n",
    "print(\"\\nA least squares regression selects the line with the lowest total sum of squared prediction errors. Thus y = 0.8*x + 0.7 is the best linear regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Lasso and Ridge regression (20pts)\n",
    "* What is the problem solved by Lasso and Ridge regression? What is the major\n",
    "difference between the two regression? Please discuss the advantages and disadvantages\n",
    "of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ridge and Lasso regression are some of the simple techniques to prevent overfitting in linear regression.\n",
      "\n",
      "Ridge regression shrinks the coefficients and it helps to reduce the model complexity and multi-collinearity (when independent variables are highly correlated)\n",
      "\n",
      "Lasso regression not only helps in reducing overfitting, but it can help us in feature selection.\n",
      "\n",
      "In ridge, the penalty is the sum of the squared coefficients\n",
      "In lasso, the penalty is the sum of the absolute values of coefficients\n",
      "\n",
      "Since Lasso Regression can exclude useless variables from equations, it is a little better than Ridge Regression at reducing the Variance in models that contain a lot of useless variables. In contrast, Ridge Regression tends to do a little better when most variables are useful.\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "Ridge and Lasso regression are some of the simple techniques to prevent overfitting in linear regression.\n",
    "\n",
    "Ridge regression shrinks the coefficients and it helps to reduce the model complexity and multi-collinearity (when independent variables are highly correlated)\n",
    "\n",
    "Lasso regression not only helps in reducing overfitting, but it can help us in feature selection.\n",
    "\n",
    "In ridge, the penalty is the sum of the squared coefficients\n",
    "In lasso, the penalty is the sum of the absolute values of coefficients\n",
    "\n",
    "Since Lasso Regression can exclude useless variables from equations, it is a little better than Ridge Regression at reducing the Variance in models that contain a lot of useless variables. In contrast, Ridge Regression tends to do a little better when most variables are useful.\n",
    "      ''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Decision Tree (30pts)\n",
    "* There are various ways to decide on the metric to choose the variable on which splitting for a node is done. Different algorithms deploy different metrices to decide which variable splits the dataset best.\n",
    "* Let’s say we have a sample of 30 records. There are two classes C1 and C2. We have three possible splits a, b, and c (see figure below). The number of records in each class is shown in every node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a Python code to measure the node impurity using Gini Index\n",
    "\n",
    "#Gini of a_Left\n",
    "aLgC1 = 3/12\n",
    "aLgC2 = 9/12\n",
    "aLGini = 1 - aLgC1**2 - aLgC2**2\n",
    "#Gini of a_Right\n",
    "aRgC1 = 17/18\n",
    "aRgC2 = 1/18\n",
    "aRGini = 1 - aRgC1**2 - aRgC2**2\n",
    "\n",
    "#Gini of b_Left\n",
    "bLgC1 = 10/15\n",
    "bLgC2 = 5/15\n",
    "bLGini = 1 - bLgC1**2 - bLgC2**2\n",
    "#Gini of b_Right\n",
    "bRgC1 = 10/15\n",
    "bRgC2 = 5/15\n",
    "bRGini = 1 - bRgC1**2 - bRgC2**2\n",
    "\n",
    "#Gini of c_Left\n",
    "cLgC1 = 19/21\n",
    "cLgC2 = 2/21\n",
    "cLGini = 1 - cLgC1**2 - cLgC2**2\n",
    "#Gini of c_Right\n",
    "cRgC1 = 1/9\n",
    "cRgC2 = 8/9\n",
    "cRGini = 1 - cRgC1**2 - cRgC2**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a Python code to measure the node impurity using Entropy Gain\n",
    "\n",
    "#Entropy Gain of a_Left\n",
    "aLeC1 = 3/12\n",
    "aLeC2 = 9/12\n",
    "aLEntropy = - aLeC1*log2(aLeC1) - aLeC2*log2(aLeC2)\n",
    "#Entropy Gain of a_Right\n",
    "aReC1 = 17/18\n",
    "aReC2 = 1/18\n",
    "aREntropy = - aReC1*log2(aReC1) - aReC2*log2(aReC2)\n",
    "\n",
    "#Entropy Gain of b_Left\n",
    "bLeC1 = 10/15\n",
    "bLeC2 = 5/15\n",
    "bLEntropy = - bLeC1*log2(bLeC1) - bLeC2*log2(bLeC2)\n",
    "#Entropy Gain of b_Right\n",
    "bReC1 = 10/15\n",
    "bReC2 = 5/15\n",
    "bREntropy = - bReC1*log2(bReC1) - bReC2*log2(bReC2)\n",
    "\n",
    "#Entropy Gain of c_Left\n",
    "cLeC1 = 19/21\n",
    "cLeC2 = 2/21\n",
    "cLEntropy = - cLeC1*log2(cLeC1) - cLeC2*log2(cLeC2)\n",
    "#Entropy Gain of c_Right\n",
    "cReC1 = 1/9\n",
    "cReC2 = 8/9\n",
    "cREntropy = - cReC1*log2(cReC1) - cReC2*log2(cReC2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a Python code to measure the node impurity using Misclassification Error\n",
    "\n",
    "#Error of a_Left\n",
    "aLmC1 = 3/12\n",
    "aLmC2 = 9/12\n",
    "aLMisclass = 1 - max(aLgC1,aLgC2)\n",
    "#Error of a_Right\n",
    "aRmC1 = 17/18\n",
    "aRmC2 = 1/18\n",
    "aRMisclass = 1 - max(aRmC1,aRmC2)\n",
    "\n",
    "#Error of b_Left\n",
    "bLmC1 = 10/15\n",
    "bLmC2 = 5/15\n",
    "bLMisclass = 1 - max(bLmC1,bLmC2)\n",
    "#Error of b_Right\n",
    "bRmC1 = 10/15\n",
    "bRmC2 = 5/15\n",
    "bRMisclass = 1 - max(bRmC1,bRmC2)\n",
    "\n",
    "#Error of c_Left\n",
    "cLmC1 = 19/21\n",
    "cLmC2 = 2/21\n",
    "cLMisclass = 1 - max(cLmC1,cLmC2)\n",
    "#Error of c_Right\n",
    "cRmC1 = 1/9\n",
    "cRmC2 = 8/9\n",
    "cRMisclass = 1 - max(cRmC1,cRmC2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node impurities:\n",
      "Gini index: a-left: 0.375; right: 0.105; b-left: 0.444; right: 0.444; c-left: 0.172; right: 0.198;\n",
      "Entropy Gain: a-left: 0.811; right: 0.31; b-left: 0.918; right: 0.918; c-left: 0.454; right: 0.503;\n",
      "Misclassification Error: a-left: 0.25; right: 0.056; b-left: 0.333; right: 0.333; c-left: 0.095; right: 0.111;\n"
     ]
    }
   ],
   "source": [
    "#Measure the node impurity using Gini Index, Entropy Gain, and Misclassification Error, respectively\n",
    "print(f'''Node impurities:\n",
    "Gini index: a-left: {aLGini}; right: {round(aRGini,3)}; b-left: {round(bLGini,3)}; right: {round(bRGini,3)}; c-left: {round(cLGini,3)}; right: {round(cRGini,3)};\n",
    "Entropy Gain: a-left: {round(aLEntropy,3)}; right: {round(aREntropy,3)}; b-left: {round(bLEntropy,3)}; right: {round(bREntropy,3)}; c-left: {round(cLEntropy,3)}; right: {round(cREntropy,3)};\n",
    "Misclassification Error: a-left: {aLMisclass}; right: {round(aRMisclass,3)}; b-left: {round(bLMisclass,3)}; right: {round(bRMisclass,3)}; c-left: {round(cLMisclass,3)}; right: {round(cRMisclass,3)};''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gini\n",
    "Gini_Childrena = 12/30 * aLGini + 18/30 * aRGini\n",
    "Gini_Childrenb = 15/30 * bLGini + 15/30 * bRGini\n",
    "Gini_Childrenc = 21/30 * cLGini + 9/30 * cRGini\n",
    "\n",
    "#entropy\n",
    "Entropy_P = - (20/30)*log2(20/30) - (10/30)*log2(10/30)\n",
    "E_remainder1=(18/30 * aREntropy)+(12/30 * aLEntropy)\n",
    "Entropy_Childrena =Entropy_P -E_remainder1\n",
    "\n",
    "E_remainder2=(15/30 * bLEntropy)+(15/30 * bREntropy)\n",
    "Entropy_Childrenb =Entropy_P -E_remainder2\n",
    "\n",
    "E_remainder3=(21/30 * cLEntropy)+(9/30 * cREntropy)\n",
    "Entropy_Childrenc =Entropy_P -E_remainder3\n",
    "\n",
    "#error\n",
    "Error_P=1 - max(20/30,10/30)\n",
    "M_remainder1=(18/30 * aLMisclass)+(12/30 * aRMisclass)\n",
    "Error_Childrena=Error_P-M_remainder1\n",
    "\n",
    "M_remainder2=(15/30 * bLMisclass)+(15/30 * bRMisclass)\n",
    "Error_Childrenb=Error_P-M_remainder2\n",
    "\n",
    "M_remainder3=(21/30 * cLMisclass)+(9/30 * cRMisclass)\n",
    "Error_Childrenc=Error_P-M_remainder3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting quality:\n",
      "Gini index: a: 0.213; b: 0.444; c: 0.18;\n",
      "Entropy Gain: a: 0.408; b: 0.0; c: 0.45;\n",
      "Misclassification Error: a: 0.2; b: 0.0; c: 0.233;\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the quality of the three splitting and report the best one (a, b, or c)\n",
    "print(f'''Splitting quality:\n",
    "Gini index: a: {round(Gini_Childrena,3)}; b: {round(Gini_Childrenb,3)}; c: {round(Gini_Childrenc,3)};\n",
    "Entropy Gain: a: {round(Entropy_Childrena,3)}; b: {Entropy_Childrenb}; c: {round(Entropy_Childrenc,3)};\n",
    "Misclassification Error: a: {round(Error_Childrena,1)}; b: {Error_Childrenb}; c: {round(Error_Childrenc,3)};''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All three methods have the same best splitting: c\n"
     ]
    }
   ],
   "source": [
    "#print out your conclusion about whether all three methods have the same best splitting or not\n",
    "print('All three methods have the same best splitting: c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: KNN (30pts)\n",
    "*  KNN: this section applies the KNN algorithm to the Iris flowers dataset.\n",
    "* The first step is to load the dataset in “iris.csv” and convert the loaded data to numbers that we can use with the mean and standard deviation calculations.\n",
    "* For this we will use the helper function load_csv() to load the file, str_column_to_float() to convert string numbers to floats and str_column_to_int() to convert the class column to integer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Iris-versicolor ID => 0\n",
      "Class Iris-virginica ID => 1\n",
      "Class Iris-setosa ID => 2\n",
      "Scores: 96.66666666666667\n",
      "Mean Accuracy: 96.667%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "        print('Class %s ID => %d' % (value, i))\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for _ in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "    distance = 0.0\n",
    "    for i in range(len(row1)-1):\n",
    "        distance += (row1[i] - row2[i])**2\n",
    "    return sqrt(distance)\n",
    "\n",
    "\n",
    "# Locate the most similar neighbors and return the list of neighbors\n",
    "def get_neighbors(train, test_row, num_neighbors):\n",
    "    distances = list()\n",
    "    for train_row in train:\n",
    "        dist = euclidean_distance(test_row, train_row)\n",
    "        distances.append((train_row, dist))\n",
    "    distances.sort(key=lambda tup: tup[1])\n",
    "    neighbors = list()\n",
    "    for i in range(num_neighbors):\n",
    "      neighbors.append(distances[i][0])\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "# Make a prediction with neighbors\n",
    "def predict_classification(train, test_row, num_neighbors):\n",
    "    neighbors = get_neighbors(train, test_row, num_neighbors)\n",
    "    output_values = [row[-1] for row in neighbors]\n",
    "    prediction = max(set(output_values), key=output_values.count)\n",
    "    return prediction\n",
    "\n",
    "# kNN Algorithm\n",
    "def k_nearest_neighbors(train, test, num_neighbors):\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        output = predict_classification(train, row, num_neighbors)\n",
    "        predictions.append(output)\n",
    "    return(predictions)\n",
    "\n",
    "\n",
    "# Test the kNN on the Iris Flowers dataset\n",
    "seed(1)\n",
    "filename = 'iris.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "    str_column_to_float(dataset, i)\n",
    "\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "num_neighbors = 5\n",
    "scores = evaluate_algorithm(dataset, k_nearest_neighbors, n_folds, num_neighbors)\n",
    "print('Scores: %s' % scores[0])\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n",
    "# the output is\n",
    "\n",
    "#The output scores is 96.66666666666667\n",
    "#The mean accuracy is 96.667%\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Pruning of decision tree (extra credit: 20pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Error (Before splitting) = 10/30 => 0.34\n",
      "Pessimistic error = (10 + 0.5)/30 = 10.5/30 => 0.35\n",
      "Training Error (After splitting) = 9/30 => 0.30\n",
      "Pessimistic error (After splitting) = (9 + 4 x 0.5)/30 = 0.37\n",
      "This tree should be pruned since 0.37>0.35\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "Training Error (Before splitting) = 10/30 => 0.34\n",
    "Pessimistic error = (10 + 0.5)/30 = 10.5/30 => 0.35\n",
    "Training Error (After splitting) = 9/30 => 0.30\n",
    "Pessimistic error (After splitting) = (9 + 4 x 0.5)/30 = 0.37\n",
    "This tree should be pruned since 0.37>0.35\n",
    "      ''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "51e4f816afd506db7bb7650607a3e2e026e40398beddcc9e11824953f6978b8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

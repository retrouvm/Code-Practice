{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 (100 + 20 points)\n",
    "* Download the file vvk160_hw5.ipynb\n",
    "* Open the file via Jupyter Notebook\n",
    "* Run the code by clicking the “Run” button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "from csv import reader\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import r2_score as r2\n",
    "import pdb #for debugging the code. Set a breakpoint by pbd.set_trace(). https://www.geeksforgeeks.org/python-debugger-python-pdb/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Linear regression (20pts)\n",
    "* We do the linear regression on three points (0.5, 1), (2, 2.5), and (3, 3). \n",
    "* Please calculate the SSEs of the four following linear regression based on the definition SSE = ∑ ( y - ŷ )^2\n",
    "* Write the Python code to output the major steps of the calculation and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = x + 0.5, SSE = 0.25\n",
      "y = x + 1, SSE = 1.5\n",
      "y = 0.8*x + 0.3 + 15, SSE = 0.54\n",
      "y = 0.8*x + 0.7, SSE = 0.06\n",
      "\n",
      "A least squares regression selects the line with the lowest total sum of squared prediction errors. Thus y = 0.8*x + 0.7 is the best linear regression\n"
     ]
    }
   ],
   "source": [
    "# Which is the best linear regression using SSE?\n",
    "\n",
    "#y = x + 0.5\n",
    "SSE1=((1 - (0.5+0.5))**2)+((2.5 - (2+0.5))**2) +((3- (3+0.5))**2)\n",
    "print(f\"y = x + 0.5, SSE = {SSE1}\")\n",
    "\n",
    "#y = x + 1\n",
    "SSE2=((1 - (0.5+1))**2)+((2.5 - (2+1))**2) +((3- (3+1))**2)\n",
    "print(f\"y = x + 1, SSE = {SSE2}\")\n",
    "\n",
    "#y = 0.8*x + 0.3\n",
    "SSE3=((1 - (0.8*0.5+0.3))**2)+((2.5 - (0.8*2+0.3))**2) +((3- (0.8*3+0.3))**2)\n",
    "print(f\"y = 0.8*x + 0.3 + 15, SSE = {round(SSE3,3)}\")\n",
    "\n",
    "#y = 0.8*x + 0.7\n",
    "SSE4=((1 - (0.8*0.5+0.7))**2)+((2.5 - (0.8*2+0.7))**2) +((3- (0.8*3+0.7))**2)\n",
    "print(f\"y = 0.8*x + 0.7, SSE = {round(SSE4,3)}\")\n",
    "\n",
    "print(\"\\nA least squares regression selects the line with the lowest total sum of squared prediction errors. Thus y = 0.8*x + 0.7 is the best linear regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Lasso and Ridge regression (20pts)\n",
    "* What is the problem solved by Lasso and Ridge regression? What is the major\n",
    "difference between the two regression? Please discuss the advantages and disadvantages\n",
    "of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ridge and Lasso regression are some of the simple techniques to prevent overfitting in linear regression.\n",
      "\n",
      "Ridge regression shrinks the coefficients and it helps to reduce the model complexity and multi-collinearity (when independent variables are highly correlated)\n",
      "\n",
      "Lasso regression not only helps in reducing overfitting, but it can help us in feature selection.\n",
      "\n",
      "In ridge, the penalty is the sum of the squared coefficients\n",
      "In lasso, the penalty is the sum of the absolute values of coefficients\n",
      "\n",
      "Since Lasso Regression can exclude useless variables from equations, it is a little better than Ridge Regression at reducing the Variance in models that contain a lot of useless variables. In contrast, Ridge Regression tends to do a little better when most variables are useful.\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "Ridge and Lasso regression are some of the simple techniques to prevent overfitting in linear regression.\n",
    "\n",
    "Ridge regression shrinks the coefficients and it helps to reduce the model complexity and multi-collinearity (when independent variables are highly correlated)\n",
    "\n",
    "Lasso regression not only helps in reducing overfitting, but it can help us in feature selection.\n",
    "\n",
    "In ridge, the penalty is the sum of the squared coefficients\n",
    "In lasso, the penalty is the sum of the absolute values of coefficients\n",
    "\n",
    "Since Lasso Regression can exclude useless variables from equations, it is a little better than Ridge Regression at reducing the Variance in models that contain a lot of useless variables. In contrast, Ridge Regression tends to do a little better when most variables are useful.\n",
    "      ''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: KNN (30pts)\n",
    "*  KNN: this section applies the KNN algorithm to the Iris flowers dataset.\n",
    "* The first step is to load the dataset in “iris.csv” and convert the loaded data to numbers that we can use with the mean and standard deviation calculations.\n",
    "* For this we will use the helper function load_csv() to load the file, str_column_to_float() to convert string numbers to floats and str_column_to_int() to convert the class column to integer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Iris-setosa ID => 0\n",
      "Class Iris-virginica ID => 1\n",
      "Class Iris-versicolor ID => 2\n",
      "Scores: 96.66666666666667\n",
      "Mean Accuracy: 96.667%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "        print('Class %s ID => %d' % (value, i))\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for _ in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "    distance = 0.0\n",
    "    for i in range(len(row1)-1):\n",
    "        distance += (row1[i] - row2[i])**2\n",
    "    return sqrt(distance)\n",
    "\n",
    "\n",
    "# Locate the most similar neighbors and return the list of neighbors\n",
    "def get_neighbors(train, test_row, num_neighbors):\n",
    "    distances = list()\n",
    "    for train_row in train:\n",
    "        dist = euclidean_distance(test_row, train_row)\n",
    "        distances.append((train_row, dist))\n",
    "    distances.sort(key=lambda tup: tup[1])\n",
    "    neighbors = list()\n",
    "    for i in range(num_neighbors):\n",
    "      neighbors.append(distances[i][0])\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "# Make a prediction with neighbors\n",
    "def predict_classification(train, test_row, num_neighbors):\n",
    "    neighbors = get_neighbors(train, test_row, num_neighbors)\n",
    "    output_values = [row[-1] for row in neighbors]\n",
    "    prediction = max(set(output_values), key=output_values.count)\n",
    "    return prediction\n",
    "\n",
    "# kNN Algorithm\n",
    "def k_nearest_neighbors(train, test, num_neighbors):\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        output = predict_classification(train, row, num_neighbors)\n",
    "        predictions.append(output)\n",
    "    return(predictions)\n",
    "\n",
    "\n",
    "# Test the kNN on the Iris Flowers dataset\n",
    "seed(1)\n",
    "filename = 'iris.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "    str_column_to_float(dataset, i)\n",
    "\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "num_neighbors = 5\n",
    "scores = evaluate_algorithm(dataset, k_nearest_neighbors, n_folds, num_neighbors)\n",
    "print('Scores: %s' % scores[0])\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n",
    "# the output is\n",
    "\n",
    "#The output scores is 96.66666666666667\n",
    "#The mean accuracy is 96.667%\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Pruning of decision tree (extra credit: 20pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Error (Before splitting) = 10/30 => 0.34\n",
      "Pessimistic error = (10 + 0.5)/30 = 10.5/30 => 0.35\n",
      "Training Error (After splitting) = 9/30 => 0.30\n",
      "Pessimistic error (After splitting) = (9 + 4 x 0.5)/30 = 0.37\n",
      "This tree should be pruned since 0.37>0.35\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "Training Error (Before splitting) = 10/30 => 0.34\n",
    "Pessimistic error = (10 + 0.5)/30 = 10.5/30 => 0.35\n",
    "Training Error (After splitting) = 9/30 => 0.30\n",
    "Pessimistic error (After splitting) = (9 + 4 x 0.5)/30 = 0.37\n",
    "This tree should be pruned since 0.37>0.35\n",
    "      ''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "51e4f816afd506db7bb7650607a3e2e026e40398beddcc9e11824953f6978b8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
